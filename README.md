# Where2Act: From Pixels to Actions for Articulated 3D Objects

![Overview](https://github.com/daerduoCarey/where2act/blob/master/images/teaser.png)

**The Proposed Where2Act Task.** Given as input an articulated 3D object, we learn to propose the actionable information for different robotic manipulation primitives (e.g. pushing, pulling): (a) the predicted actionability scores over pixels; (b) the proposed interaction trajectories, along with (c) their success likelihoods, for a selected pixel highlighted in red. We show two high-rated proposals (left) and two with lower scores (right) due to interaction orientations and potential robot-object collisions.

## Introduction
We learn to predict per-pixel actionable information for manipulating 3D articulated objects.

## About the paper

Our team: 
[Kaichun Mo](https://cs.stanford.edu/~kaichun),
[Leonidas J. Guibas](https://geometry.stanford.edu/member/guibas/),
[Mustafa Mukadam](http://www.mustafamukadam.com/),
[Abhinav Gupta](http://www.cs.cmu.edu/~abhinavg/),
and [Shubham Tulsiani](https://shubhtuls.github.io/)
from 
Stanford University and FaceBook AI Research.

Arxiv Version: 

Project Page: https://cs.stanford.edu/~kaichun/where2act

## Citations
    @article{mo2020where2act,
          Author={Mo, Kaichun and Guibas, Leonidas and Mukadam, Mustafa and Gupta, Abhinav and Tulsiani, Shubham},
          Title={Where2Act: From Pixels to Actions for Articulated 3D Objects},
          Year={2020},
          Eprint={},
    }

## About this repository

This repository provides data and code as follows.


```
    data/                   # contains data, models, results, logs
    code/                   # contains code and scripts
         # please follow `code/README.md` to run the code
    stats/                  # contains helper statistics
```

## Questions

Please post issues for questions and more helps on this Github repo page. We encourage using Github issues instead of sending us emails since your questions may benefit others.

## License

MIT Licence

## Updates

* [Jan xxx, 2021] Data and Code released.

